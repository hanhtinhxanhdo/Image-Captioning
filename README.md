# Image Captioning with ResNet50 Encoder and RNN Decoder
This repository hosts an Image Captioning project that utilizes a ResNet50-based encoder and a Recurrent Neural Network (RNN) decoder to generate descriptive captions for images. The project is trained on the COCO (Common Objects in Context) dataset, aiming to create a model capable of understanding complex visual scenes and expressing them in natural language.

## Key Features
- **ResNet50 Encoder**: The project employs the ResNet50 architecture as the image encoder, extracting high-level features from input images. This enables the model to understand intricate details and context within the visual data.
- **RNN Decoder**: A Recurrent Neural Network (RNN) serves as the decoder, generating sequential captions based on the encoded image features. This approach allows the model to capture temporal dependencies and create coherent and contextually relevant descriptions.
- **COCO Dataset Training**: The model is trained on the COCO dataset, a widely used benchmark for image captioning tasks. The dataset encompasses a diverse range of images, making the trained model versatile in describing various visual scenes.

## Acknowledgments
- The project is built upon the foundation of the COCO dataset.
- Inspiration and references from seminal papers in the field of image captioning.
